version: 4
data:
  attachedData:
    trivet:
      testSuites: []
      version: 1
  graphs:
    7iHnWEmcH0QvWCxWPIdxa:
      metadata:
        description: ""
        id: 7iHnWEmcH0QvWCxWPIdxa
        name: "Function Call: replyToUser"
      nodes:
        '[BI8U0sdUOiTsJJbFXXghs]:graphInput "Graph Input"':
          data:
            dataType: boolean
            id: shouldReply
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Object" MDlYAGqqL4vQE94CVZs5s/shouldReply
          visualData: 484/767/330/6//
        '[MDlYAGqqL4vQE94CVZs5s]:object "Object"':
          data:
            jsonTemplate: |-
              {
                "reply": "{{reply}}",
                "helpfulness": {{helpfulness}},
                "shouldReply": {{shouldReply}},
                "internalThoughts": "{{internalThoughts}}"
              }
          outgoingConnections:
            - output->"Graph Output" wJ5GxvvY-8mzod4FPVVOu/value
          visualData: 981/568/230/15//
        '[VJfRwAJ9RW7TNzVOvR-Cr]:graphInput "Graph Input"':
          data:
            dataType: string
            id: reply
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Object" MDlYAGqqL4vQE94CVZs5s/reply
          visualData: 484/367/330/6//
        '[hyaya4jtQdZ-MeDQWhuNq]:graphInput "Graph Input"':
          data:
            dataType: number
            id: helpfulness
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Object" MDlYAGqqL4vQE94CVZs5s/helpfulness
          visualData: 484/567/330/6//
        '[poMHE7ObF-MnXO9QbZhas]:graphInput "Graph Input"':
          data:
            dataType: string
            id: internalThoughts
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Object" MDlYAGqqL4vQE94CVZs5s/internalThoughts
          visualData: 484/967/330/6//
        '[wJ5GxvvY-8mzod4FPVVOu]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1286/621/330/16//
    DIQ-F1DT6fPn3RQzAUtce:
      metadata:
        description: ""
        id: DIQ-F1DT6fPn3RQzAUtce
        name: Embed Conversation
      nodes:
        '[FeZkCd5IUWo-qRUH3ssHG]:text "Text"':
          data:
            text: >-
              Rivet is a powerful Integrated Development Environment (IDE) and
              library designed for creating AI agents using a visual,
              graph-based interface. It is made up of two main components: the
              Rivet Application and the Rivet Core/Rivet Node. The Rivet
              Application is an editor that allows you to create complex AI
              prompt chains and agents. It comes with a suite of tools for
              designing and enhancing AI agents, such as a prompt designer,
              variations on nodes for A/B testing, and integrated testing to
              ensure your graphs work as expected for all inputs.


              Rivet Core/Rivet Node, on the other hand, are TypeScript libraries that allow you to execute projects generated by the Rivet Application. They provide a simple API for integrating Rivet with your application. This makes it easy to integrate Rivet's AI capabilities into your existing projects. Once you've created a graph in the Rivet App, you can execute it within your application like a function call.


              One of the key features of Rivet is its node-based editor. This enables you to create, configure, and debug complex AI prompt chains and AI agent chains visually. This approach makes it easier to understand the flow of data and the state of your AI agent at any point in time. The editor allows you to view the input and output of every node, as well as AI responses in real-time, making it simple to identify and fix issues.


              Rivet also offers live debugging of AI chains as they run, allowing you to monitor the state of your AI agent in real-time and quickly identify any issues that may arise. It also supports remote debugging, allowing you to debug AI chains running on a remote server. This is useful for debugging AI agents that are running in a production environment.


              Lastly, Rivet features a library of node types to execute common functionality for nodes. Some essential node types include Text, Chat, Match, Loop Controller, Extract YAML, Extract JSON, Chunk, Trim Chat Messages, and External Call. These nodes can be connected together using wires, allowing data to flow between them. Documentation for all nodes can be found in the Node Reference.
          outgoingConnections:
            - output->"Text" PChwvgCCnAsnmNSEb86Yh/rivet
          visualData: 1003/454/330/12//
        '[N9JTvqa_63bM75lIf8gR2]:join "Join"':
          data:
            flatten: true
            joinString: "\n"
          outgoingConnections:
            - output->"Text" PChwvgCCnAsnmNSEb86Yh/conversation
            - output->"Text" bXUwO7wInK4rHmcDFxzCF/conversation
          visualData: 1277/775/180/11//
        '[PChwvgCCnAsnmNSEb86Yh]:text "Text"':
          data:
            text: >-
              {{rivet}}


              This conversation happened on the Rivet Discord:


              <conversation>

              {{conversation}}

              </conversation>


              ## Important notes:


              Andy (Snea) is the creator of Rivet. What he said should be treated as authoritative when it comes to Rivet. Every other user is just a user of Rivet asking questions or providing input/conversation in discord. If Andy/Snea replies, that should be seen as authoritative statements about Rivet.


              ## Instructions


              Summarize this conversation in one paragraph. The chat and summary will be combined into a vector database with embeddings, so make a useful summary with key words from the conversation.


              Reply with your summary.


              ## Followup


              After your summary, be creative and come up with a list of questions that people may ask, which may be answered by this conversation. If there are no questions that may be answered by this conversation, you may omit this step.
          outgoingConnections:
            - output->"Chat" kZSMsakRgVHiufJTr4ymb/prompt
          visualData: 1495/542/330/13//
        '[Sl0EpbpufkOEWW23DndUU]:destructure "Destructure"':
          data:
            paths:
              - $.timestamp
              - $.userName
              - $.content
          isSplitRun: true
          outgoingConnections:
            - match_0->"Text" UylcRfmRTi_G9as9zsbXm/timestamp
            - match_1->"Text" UylcRfmRTi_G9as9zsbXm/user
            - match_2->"Text" UylcRfmRTi_G9as9zsbXm/content
          splitRunMax: 100
          visualData: 540/791/280/8//
        '[UylcRfmRTi_G9as9zsbXm]:text "Text"':
          data:
            text: "* [{{user}}] ({{timestamp}}) {{content}}"
          isSplitRun: true
          outgoingConnections:
            - output->"Join" N9JTvqa_63bM75lIf8gR2/input1
          splitRunMax: 999
          visualData: 891/776/330/9//
        '[bXUwO7wInK4rHmcDFxzCF]:text "Text"':
          data:
            text: |-
              {{conversation}}

              {{summary}}
          outgoingConnections:
            - output->"Get Embedding" gLGANLKD39W8shovNmkBZ/input
          visualData: 2215/770/330/18//
        '[gLGANLKD39W8shovNmkBZ]:getEmbedding "Get Embedding"':
          data:
            integration: openai
            model: text-embedding-3-small
            useIntegrationInput: false
          outgoingConnections:
            - embedding->"Graph Output" tdWCZDK1YwNplz-wIjy0a/value
          visualData: 2586/801/280/20//
        '[kZSMsakRgVHiufJTr4ymb]:chat "Chat"':
          data:
            additionalParameters: []
            cache: false
            enableFunctionUse: false
            maxTokens: 1024
            modalitiesIncludeAudio: false
            modalitiesIncludeText: false
            model: gpt-4o-mini
            outputUsage: false
            parallelFunctionCalling: true
            reasoningEffort: medium
            stop: ""
            temperature: 0.5
            top_p: 1
            useAdditionalParametersInput: false
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePredictedOutput: false
            usePresencePenaltyInput: false
            useReasoningEffortInput: false
            useServerTokenCalculation: true
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Graph Output" qCDOngwyUTg2vRDjt0xqK/value
            - response->"Text" bXUwO7wInK4rHmcDFxzCF/summary
          visualData: 1880/694/230/15//
        '[qCDOngwyUTg2vRDjt0xqK]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: summary
          visualData: 2947/600/330/23//
        '[tdWCZDK1YwNplz-wIjy0a]:graphOutput "Graph Output"':
          data:
            dataType: vector
            id: vector
          visualData: 2959/908/330/22//
        '[tt7DVnw4vo4Ri4Ql5e7tn]:graphInput "Graph Input"':
          data:
            dataType: object[]
            id: conversation_messages
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Destructure" Sl0EpbpufkOEWW23DndUU/object
          visualData: 180/759/330/5//
        '[vUAi04ONFNSjXe7cLxtAz]:object "Object"':
          data:
            jsonTemplate: "[

              \  {

              \    \"id\": \"1165858665023164506\",

              \    \"content\": \"hi, can you use open source LLM
              models?\",

              \    \"timestamp\": \"2023-10-23 03:46:06\",

              \    \"userId\": \"752223202641576138\",

              \    \"userName\": \"C.luna80\"

              \  },

              \  {

              \    \"id\": \"1165858892195045456\",

              \    \"content\": \"sure with oobabooga or lm
              studio\",

              \    \"timestamp\": \"2023-10-23 03:47:00\",

              \    \"userId\": \"110875316037091328\",

              \    \"userName\": \"Snea\"

              \  },

              \  {

              \    \"id\": \"1165861824185507910\",

              \    \"content\": \"Look, I made a graph!\",

              \    \"timestamp\": \"2023-10-23 03:58:39\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  },

              \  {

              \    \"id\": \"1165862006310572033\",

              \    \"content\": \"oh no\",

              \    \"timestamp\": \"2023-10-23 03:59:22\",

              \    \"userId\": \"110875316037091328\",

              \    \"userName\": \"Snea\"

              \  },

              \  {

              \    \"id\": \"1165862132139708537\",

              \    \"content\": \"It's +2k nodes\",

              \    \"timestamp\": \"2023-10-23 03:59:52\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  },

              \  {

              \    \"id\": \"1165862162330288128\",

              \    \"content\": \"But it works!\",

              \    \"timestamp\": \"2023-10-23 04:00:00\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  },

              \  {

              \    \"id\": \"1165862281452728320\",

              \    \"content\": \"It freezes for 20-30 seconds
              when you hit run though\",

              \    \"timestamp\": \"2023-10-23 04:00:28\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  },

              \  {

              \    \"id\": \"1165862344144994384\",

              \    \"content\": \"oof what are you trying to
              do?\",

              \    \"timestamp\": \"2023-10-23 04:00:43\",

              \    \"userId\": \"110875316037091328\",

              \    \"userName\": \"Snea\"

              \  },

              \  {

              \    \"id\": \"1165862501695623228\",

              \    \"content\": \"It reads up to 320 files and
              appends them to a dataset\",

              \    \"timestamp\": \"2023-10-23 04:01:21\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  },

              \  {

              \    \"id\": \"1165862554724208711\",

              \    \"content\": \"It's enough for the entire Rivet
              Documentation\",

              \    \"timestamp\": \"2023-10-23 04:01:33\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  },

              \  {

              \    \"id\": \"1165862617160613908\",

              \    \"content\": \"pretty sure you can just do that
              with splitting\",

              \    \"timestamp\": \"2023-10-23 04:01:48\",

              \    \"userId\": \"110875316037091328\",

              \    \"userName\": \"Snea\"

              \  },

              \  {

              \    \"id\": \"1165862667785883658\",

              \    \"content\": \"\",

              \    \"timestamp\": \"2023-10-23 04:02:00\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  },

              \  {

              \    \"id\": \"1165862749365080065\",

              \    \"content\":
              \"https://discord.com/channels/1149376303070466110/11493827139047\
              46597/1165279402154786908\",

              \    \"timestamp\": \"2023-10-23 04:02:20\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  },

              \  {

              \    \"id\": \"1165862856647004210\",

              \    \"content\": \"\\\"Help! I still don't
              understand splitting! 😭\\\"\",

              \    \"timestamp\": \"2023-10-23 04:02:45\",

              \    \"userId\": \"448864484631773187\",

              \    \"userName\": \"Egalitaristen\"

              \  }

              ]"
          outgoingConnections:
            - output->"Graph Input" tt7DVnw4vo4Ri4Ql5e7tn/default
          visualData: -66/549/230/3//
    DYKDBueiTSnn9xhpjq6Cb:
      metadata:
        description: ""
        id: DYKDBueiTSnn9xhpjq6Cb
        name: Messages List to String
      nodes:
        '[2RQ7vl8P_2CbaCUm3Iovh]:text "Text"':
          data:
            text: "* [message-id:{{id}}] [{{username}}] ({{timestamp}}) [{{replyTo}}]
              {{message}}"
          isSplitRun: true
          outgoingConnections:
            - output->"Join" Km7eXNKVjWcQZk3nEgkgM/input1
          splitRunMax: 1000
          visualData: 1097.0888619775685/482.8987746066194/330/5//
        '[2tbQBCAinLmqUYv55zIi2]:destructure "Destructure"':
          data:
            paths:
              - $.content
              - $.timestamp
              - $.user.displayName
              - $.replyTo
              - $.id
          isSplitRun: true
          outgoingConnections:
            - match_0->"Text" 2RQ7vl8P_2CbaCUm3Iovh/message
            - match_1->"Text" 2RQ7vl8P_2CbaCUm3Iovh/timestamp
            - match_2->"Text" 2RQ7vl8P_2CbaCUm3Iovh/username
            - match_3->"If/Else" Xfk12v6Xk22r61JKczT_Q/if
            - match_3->"Text" XZuY6fQTt-V3SYtakT0Dk/name
            - match_4->"Text" 2RQ7vl8P_2CbaCUm3Iovh/id
          splitRunMax: 1000
          visualData: 76.58825593006588/502.343432864631/280/10//
        '[Es4aXLRpikRrqHoV7LoBM]:text "Text"':
          data:
            text: |-
              The following messages occurred in the {{channel_id}} channel:

              {{messages}}
          outgoingConnections:
            - output->"Graph Output" zEOgsiWbpklRVgem-5DAL/value
          visualData: 1773.124419935913/431.07829673934935/330/null//
        '[Km7eXNKVjWcQZk3nEgkgM]:join "Join"':
          data:
            flatten: true
            joinString: "\n"
          outgoingConnections:
            - output->"Text" Es4aXLRpikRrqHoV7LoBM/messages
          visualData: 1466.8122862129364/486.1562497100148/180/6//
        '[M96XSq_Ejq2I3lM-6DBFJ]:graphInput "Graph Input"':
          data:
            dataType: object[]
            id: messages
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Destructure" 2tbQBCAinLmqUYv55zIi2/object
            - data->"Pop" mI2ASe_o593N5Ag2gM789/array
          visualData: -377.4194509347184/492.16134858839206/330/15//
        '[XZuY6fQTt-V3SYtakT0Dk]:text "Text"':
          data:
            text: replying to {{name}}
          isSplitRun: true
          outgoingConnections:
            - output->"If/Else" Xfk12v6Xk22r61JKczT_Q/true
          splitRunMax: 100
          visualData: 178.4150126631589/833.6849931925921/330/14//
        '[Xfk12v6Xk22r61JKczT_Q]:ifElse "If/Else"':
          data:
            unconnectedControlFlowExcluded: true
          isSplitRun: true
          outgoingConnections:
            - output->"Text" 2RQ7vl8P_2CbaCUm3Iovh/replyTo
          splitRunMax: 100
          visualData: 660.817630935963/763.5613043208562/205/null//
        '[mI2ASe_o593N5Ag2gM789]:pop "Pop"':
          outgoingConnections:
            - lastItem->"Destructure" n9Q_c-63dawcJLYHrCH2l/object
          visualData: 94.99200348816248/259.39616191544405/230/20//
        '[n9Q_c-63dawcJLYHrCH2l]:destructure "Destructure"':
          data:
            paths:
              - $.channelId
          outgoingConnections:
            - match_0->"Text" Es4aXLRpikRrqHoV7LoBM/channel_id
          visualData: 432.3111275435821/277.53159869261714/280/21//
        '[z2K85ivI1X6ILw985asX8]:text "Text"':
          data:
            text: not replying
          outgoingConnections:
            - output->"If/Else" Xfk12v6Xk22r61JKczT_Q/false
          visualData: 23.65928549794844/1038.0109142154088/330/12//
        '[zEOgsiWbpklRVgem-5DAL]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 2174.8451515770466/446.52770868991297/330/19//
    TmT0Ru-qIpo5jyAsvrzFq:
      metadata:
        description: ""
        id: TmT0Ru-qIpo5jyAsvrzFq
        name: Helpful Message From Past Conversations
      nodes:
        '[2cxPbMPSoHsOBttxbQfEF]:toTree "To Tree"':
          data:
            childrenProperty: children
            format: "{{path}}"
            useSortAlphabetically: true
          outgoingConnections:
            - tree->"Text" FA7RjrEMsb2Sqbw1_sgbE/file_tree
          visualData: 574.131915540522/247.29386857014327/330/46//
        '[3GdlHogl2PM56pjMg6CEx]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: internalThoughts
          visualData: 8458.636097302091/-217.9433857681133/330/79//
        '[6b2YD8MwlO8DEljH0iDF6]:delegateFunctionCall "Delegate Function Call"':
          data:
            handlers:
              - key: readRivetFile
                value: qjZ8pqtpef9Y0qtsiVzum
          isSplitRun: true
          outgoingConnections:
            - message->"Assemble Prompt" rOL6VbCiarjjnfrSnJ2Si/message2
          visualData: 4468.920324694919/679.7490583306187/442.9055576925157/76//
        '[AD5oG59KEgJVykz_KHGG4]:match "Match"':
          data:
            cases:
              - replyToUser
          outgoingConnections:
            - case1->"Coalesce" d0mVbF4PrtGDYaT70HrdJ/input1
            - unmatched->"Delegate Function Call"
              6b2YD8MwlO8DEljH0iDF6/function-call
          visualData: 3946.5990179148903/278.68024734227964/280/70//
        '[BuZ9gxK8bT5QbnTEQFx_x]:extractObjectPath "Extract Object Path"':
          data:
            path: $.children[0].children[0].children[0].children[0].children[0]
            usePathInput: false
          outgoingConnections:
            - match->"To Tree" 2cxPbMPSoHsOBttxbQfEF/objects
          visualData: 65.88638026639983/277.91378561546776/280/61//
        '[CUY743m5jVWBw7recCWMT]:gptFunction "GPT Function"':
          data:
            description: >-
              Ends your pondering and replies to the user. Or ends your
              pondering and does not reply to the user, because you have nothing
              useful to say.


              ## Internal Thoughts


              In your internal thoughts, talk about whether the message is part of a conversation, whether you should butt-in, whether this is a brand new message or question unrelated, etc. You are most helpful to brand new messages not part of a conversation.


              ## Helpfulness Score


              With a score from 0 to 10, rate how helpful of a response you can give. A 10 means that the response you give is extremely helpful and can likely answer the user's question, if they have one. A 0 means you have no idea and can provide nothing to the conversation.


              Even if your reply is negative, it may be very helpful! For example, "Rivet does not support that" is a helpful reply.
            name: replyToUser
            schema: >-
              
              {
                "type": "object",
                "properties": {
                  "internalThoughts": {
                    "type": "string",
                    "description": "These thoughts are to yourself"
                  },
                  "shouldMessage": {
                    "type": "boolean",
                    "description": "Indicates whether a message should be sent"
                  },
                  "reply": {
                    "type": "string",
                    "description": "This is your text reply to the user"
                  },
                  "helpfulness": {
                    "type": "integer",
                    "description": "A rating of helpfulness",
                    "minimum": 1,
                    "maximum": 10
                  }
                },
                "required": ["internalThoughts", "shouldMessage", "reply", "helpfulness"],
                "additionalProperties": false
              }
            text: ""
          outgoingConnections:
            - function->"Array" grjGx_dLGo-EbghJwxXHr/input1
            - function->"Array" poJFOQ2N1oIlq8-1C0xFN/input1
          visualData: 2132.5021418336246/1144.4343642855567/280/65//
        '[CjCP5tAVoFnV1_UgLHGsp]:graphInput "Graph Input"':
          data:
            dataType: object[]
            id: conversations
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Extract Object Path" _N9pD3_tIORVGCL7A0omw/object
          visualData: -222.58450545316754/609.0853481413917/330/42//
        '[E8uylVNG2RiTJoIY0hGHB]:object "Object"':
          data:
            jsonTemplate: "[{\"messages\":[{\"id\":\"1154222655872970862\",\"content\":\"I
              may have stumbled upon one of the most verbose but logical
              open-source models yet. I just asked one question
              haha\",\"timestamp\":\"2023-09-21
              01:08:45\",\"userId\":\"991333885000568852\",\"userName\":\"Sever\
              ian\"},{\"id\":\"1154222963562909856\",\"content\":\"Also, for
              others who want to use a local LLM but not Oobabooga (I seem to
              always have issues with my M1), here is a graph template that uses
              LM Studio for inference instead ***Custom endpoint set in system
              settings to POST http from LM Studio
              server***\",\"timestamp\":\"2023-09-21
              01:09:58\",\"userId\":\"991333885000568852\",\"userName\":\"Sever\
              ian\"},{\"id\":\"1154232109356036136\",\"content\":\"does this use
              a custom endpoint or something?\",\"timestamp\":\"2023-09-21
              01:46:19\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154234881312833628\",\"content\":\"It uses the
              custom endpoint in the system settings, specifically the POST http
              from LM Studio\",\"timestamp\":\"2023-09-21
              01:57:20\",\"userId\":\"991333885000568852\",\"userName\":\"Sever\
              ian\"},{\"id\":\"1154234974644486155\",\"content\":\"might want to
              put that endpoint in the chat node too for the shared
              graph\",\"timestamp\":\"2023-09-21
              01:57:42\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154235034262327377\",\"content\":\"pretty cool that
              you got it working that way\",\"timestamp\":\"2023-09-21
              01:57:56\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154235290530099257\",\"content\":\"I had tested
              both ways and it seemed to work with the system settings endpoint
              alone so I decided to cut the extra node. Would that potentially
              come back to haunt me for some
              reason?\",\"timestamp\":\"2023-09-21
              01:58:57\",\"userId\":\"991333885000568852\",\"userName\":\"Sever\
              ian\"},{\"id\":\"1154235370125398066\",\"content\":\"no I mean for
              sharing, it'll error for people because they don't have that set
              in the system settings\",\"timestamp\":\"2023-09-21
              01:59:16\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154235590485753926\",\"content\":\"Ahhh duh that
              makes sense. My bad! I'll redo it with more clear instructions in
              the AM and repost\",\"timestamp\":\"2023-09-21
              02:00:09\",\"userId\":\"991333885000568852\",\"userName\":\"Sever\
              ian\"},{\"id\":\"1154495325977989120\",\"content\":\"wait, you're
              going all nested, but then you took out 'presence penalty'? Also,
              I know one person on earth who has found a possible use for
              logit_bias, so I'll have you know it has been used
              before!\",\"timestamp\":\"2023-09-21
              19:12:15\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1154495375839858790\",\"content\":\"hahaha\",\"time\
              stamp\":\"2023-09-21
              19:12:27\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154495411629858948\",\"content\":\"I didn't take
              that out\",\"timestamp\":\"2023-09-21
              19:12:35\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154495513358512290\",\"content\":\"mmm... it's
              still too fast for me to read.\",\"timestamp\":\"2023-09-21
              19:12:59\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1154495549282721835\",\"content\":\"the chat node
              was just getting too many fields is
              all\",\"timestamp\":\"2023-09-21
              19:13:08\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154495573722927104\",\"content\":\"for
              sure.\",\"timestamp\":\"2023-09-21
              19:13:14\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1154496428350132234\",\"content\":\"There's a lot of
              UI that just sort of looks like the OpenAI playground to start
              out. E.g. I saw the salesforce AI admin tool recently, and it's
              like 90% \\\"playground\\\", and I don't think most SF admins and
              sales folks are tweaking their top_p settings for generating sales
              follow ups.\",\"timestamp\":\"2023-09-21
              19:16:38\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1154496512290738227\",\"content\":\"yeah a lot of
              simple wrappers around GPT out there\",\"timestamp\":\"2023-09-21
              19:16:58\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154497041033076747\",\"content\":\"like this
              example of \\\"Prompt Builder\\\". Maybe that's just supposed to
              be a playground clone tho'.\",\"timestamp\":\"2023-09-21
              19:19:04\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1154497133211287562\",\"content\":\"well that's
              totally what Prompt Designer is meant to be in
              rivet\",\"timestamp\":\"2023-09-21
              19:19:26\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154497152211501067\",\"content\":\"I don't hide
              that\",\"timestamp\":\"2023-09-21
              19:19:30\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154504330863464468\",\"content\":\"\",\"timestamp\
              \":\"2023-09-21
              19:48:02\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154532574975242401\",\"content\":\"<@7143410868305\
              26506>
              https://github.com/Ironclad/rivet/releases/tag/app-v1.4.0-beta1\",\
              \"timestamp\":\"2023-09-21
              21:40:16\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154532665047916597\",\"content\":\"trying!\",\"tim\
              estamp\":\"2023-09-21
              21:40:37\",\"userId\":\"714341086830526506\",\"userName\":\"wayne\
              \"},{\"id\":\"1154534553831100426\",\"content\":\"i don't think
              the header is saving (when i type in api-key, exit, and re-open to
              advanced, it's no long there)\",\"timestamp\":\"2023-09-21
              21:48:07\",\"userId\":\"714341086830526506\",\"userName\":\"wayne\
              \"},{\"id\":\"1154535708573319229\",\"content\":\"(on existing
              chat node)\",\"timestamp\":\"2023-09-21
              21:52:43\",\"userId\":\"714341086830526506\",\"userName\":\"wayne\
              \"},{\"id\":\"1154535715925917787\",\"content\":\"ah alright...
              just realized those will be stored in the project file if you do
              that which probably isn't a good idea to put the api key in the
              chat node anyway lol\",\"timestamp\":\"2023-09-21
              21:52:44\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154535808552935548\",\"content\":\"try the general
              settings\",\"timestamp\":\"2023-09-21
              21:53:06\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154536843828793464\",\"content\":\"haha
              true\",\"timestamp\":\"2023-09-21
              21:57:13\",\"userId\":\"714341086830526506\",\"userName\":\"wayne\
              \"},{\"id\":\"1154536847024857129\",\"content\":\"hmm
              thoughts\",\"timestamp\":\"2023-09-21
              21:57:14\",\"userId\":\"714341086830526506\",\"userName\":\"wayne\
              \"},{\"id\":\"1154536853811232788\",\"content\":\"What about a
              local config file with list of:\\n\\n- end points, deployment
              names, and api keys?\",\"timestamp\":\"2023-09-21
              21:57:16\",\"userId\":\"714341086830526506\",\"userName\":\"wayne\
              \"},{\"id\":\"1154538001905500230\",\"content\":\"I await your
              fantastic PR\",\"timestamp\":\"2023-09-21
              22:01:49\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154542677929504858\",\"content\":\"I love Rivet -
              can I contribute to help deploy rivet-core through sponsorship? 🙂
              multi-azure and the token size mod (together plus the if/else node
              - powerful)\",\"timestamp\":\"2023-09-21
              22:20:24\",\"userId\":\"714341086830526506\",\"userName\":\"wayne\
              \"},{\"id\":\"1154547390238429297\",\"content\":\"kind of
              confused, what do you mean?\",\"timestamp\":\"2023-09-21
              22:39:08\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154548149910777958\",\"content\":\"Id like to
              deploy rivet-core but the two things above are gating factors 🙂
              would love to sponsor / donate to help development on unblocking
              those 🙂\",\"timestamp\":\"2023-09-21
              22:42:09\",\"userId\":\"714341086830526506\",\"userName\":\"wayne\
              \"},{\"id\":\"1154548496846823494\",\"content\":\"gotcha,
              hmm\",\"timestamp\":\"2023-09-21
              22:43:32\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154725247799545866\",\"content\":\"will the created
              agents work only in this ide or can they be integrated into my
              applications?\",\"timestamp\":\"2023-09-22
              10:25:52\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154725380847054960\",\"content\":\"🧐\",\"timestamp\
              \":\"2023-09-22
              10:26:24\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154725604827074642\",\"content\":\"Will I be able
              to connect third-party plugins such as
              ElevenLabs?\",\"timestamp\":\"2023-09-22
              10:27:17\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154725795336573050\",\"content\":\"Can I integrate
              my agent into the Flutter app?\",\"timestamp\":\"2023-09-22
              10:28:03\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154806174303145994\",\"content\":\"My understanding
              is that Rivet is designed to integrate into applications
              https://rivet.ironcladapp.com/docs/api-reference/getting-started-\
              integration\\nI mostly use it as a agent/chain prompt playground.
              There's instructions for developing 3rd party plugins. I don't
              know what Flutter is, but to integrate a Rivet chain, I assume it
              needs to be an app that you've written and
              deploy.\",\"timestamp\":\"2023-09-22
              15:47:27\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1154829238470463608\",\"content\":\"this would
              probably need someone to write an elevenlabs plugin I
              think\",\"timestamp\":\"2023-09-22
              17:19:06\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154906782376927323\",\"content\":\"There is an API
              there, do you think there is a possibility to connect it to
              IDE?\",\"timestamp\":\"2023-09-22
              22:27:14\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154914518602883152\",\"content\":\"not sure I
              understand what you're asking\",\"timestamp\":\"2023-09-22
              22:57:58\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154914692167381103\",\"content\":\"i
              mean\\nhttps://docs.elevenlabs.io/api-reference/quick-start/intro\
              duction\",\"timestamp\":\"2023-09-22
              22:58:39\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154914701088673903\",\"content\":\"api\",\"timesta\
              mp\":\"2023-09-22
              22:58:42\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154914785905873079\",\"content\":\"yeah, someone
              will need to write a plugin to integrate that with
              rivet\",\"timestamp\":\"2023-09-22
              22:59:02\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154917915615821874\",\"content\":\"<@1108753160370\
              91328> Is there any way to add an agent to access the user's
              database to read information?\",\"timestamp\":\"2023-09-22
              23:11:28\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154918030850138153\",\"content\":\"that's something
              you'll have to implement with external call nodes and rivet
              integrated into your node application\",\"timestamp\":\"2023-09-22
              23:11:55\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154918127923109950\",\"content\":\"there's a
              possibility?\",\"timestamp\":\"2023-09-22
              23:12:19\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154918138073337906\",\"content\":\"right?\",\"time\
              stamp\":\"2023-09-22
              23:12:21\",\"userId\":\"263405532277112832\",\"userName\":\"Hakon\
              \"},{\"id\":\"1154918233762172928\",\"content\":\"anything's
              possible with enough code 🤷\",\"timestamp\":\"2023-09-22
              23:12:44\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154926333688828065\",\"content\":\"The most
              important part of any slide deck is going to DALLE and making
              goofy images. I'm going to present on Rivet, so I needed little
              robots working together with wires.\",\"timestamp\":\"2023-09-22
              23:44:55\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1154926447190868108\",\"content\":\"those look great
              lol\",\"timestamp\":\"2023-09-22
              23:45:22\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1154926619861991504\",\"content\":\"I\\\"m kind of
              afraid of DALLE 3 coming out soon\",\"timestamp\":\"2023-09-22
              23:46:03\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1154926786707193947\",\"content\":\"b/c dalle 2 is
              already pretty darned good. Well, Bing image creator
              here\",\"timestamp\":\"2023-09-22
              23:46:43\",\"userId\":\"725506683765260368\",\"userName\":\"leop\
              \"},{\"id\":\"1156496011888443422\",\"content\":\"Couldn't you
              calt lvia http header\\n?\",\"timestamp\":\"2023-09-27
              07:42:15\",\"userId\":\"956006411089883146\",\"userName\":\"CoinC\
              artel\"},{\"id\":\"1156618636572766218\",\"content\":\"say
              again?\",\"timestamp\":\"2023-09-27
              15:49:31\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"}],\"distance\":1.2289913892745972},{\"messages\":[{\"id\":\"12\
              26867523878649917\",\"content\":\"I wrote a local proxy (warp_pipe
              on github) that frontends a lot of services including local ones
              like Ollama and added in a compatibility layer for function
              calling where it isn't supported... function calling with a Q5K_M
              mistral seems to work pretty well in
              Rivet:\",\"timestamp\":\"2024-04-08
              12:13:31\",\"userId\":\"476038226528960512\",\"userName\":\"Batte\
              ryShark\"},{\"id\":\"1227990705771778170\",\"content\":\"Very nice
              use case! Thanks! This morning I use for first time rivet and I
              create a graph that start with a user question (ad example a
              recruiter) and I implemented a simple framework prompt with a role
              (system prompt), an action (extendible, for now only one), a
              content of text readed by a file (focus content of prompt of
              recruiter, ad example my portfolio dataset), and finally the
              objective, that is tuned where I need. The chat node
              \\\"oracol\\\" 😅, using a gpt4-turbo, give a response about
              contest and, if the prompt is out of context, reply with a cortesy
              denied. All response I append into a dataset of rivet. Cost of
              token 0.050 cents each. Rivet is
              amazing!!\",\"timestamp\":\"2024-04-11
              14:36:39\",\"userId\":\"521776882061017088\",\"userName\":\"kmtnck\
              \"}],\"distance\":1.231770634651184},{\"messages\":[{\"id\":\"115\
              8053361275379783\",\"content\":\"<@110875316037091328> update all
              my stuff is working, thank you for support and
              patience.\",\"timestamp\":\"2023-10-01
              14:50:36\",\"userId\":\"262031590362710017\",\"userName\":\"pi\"}\
              ,{\"id\":\"1158093552027443390\",\"content\":\"yay great to
              hear!\",\"timestamp\":\"2023-10-01
              17:30:19\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"}],\"distance\":1.2340686321258545},{\"messages\":[{\"id\":\"11\
              76960678599327754\",\"content\":\"good day rivet
              family!\",\"timestamp\":\"2023-11-22
              19:01:32\",\"userId\":\"536389581273038868\",\"userName\":\"annias\
              \"},{\"id\":\"1176960706638258336\",\"content\":\"\",\"timestamp\
              \":\"2023-11-22
              19:01:39\",\"userId\":\"536389581273038868\",\"userName\":\"annias\
              \"}],\"distance\":1.2342679500579834},{\"messages\":[{\"id\":\"11\
              51910532429926511\",\"content\":\"Hey!\",\"timestamp\":\"2023-09-\
              14
              16:01:12\",\"userId\":\"213651890746032128\",\"userName\":\"Vasek\
              \"},{\"id\":\"1151914385074307153\",\"content\":\"Great to be
              here! Super excited for Rivet's
              future.\",\"timestamp\":\"2023-09-14
              16:16:30\",\"userId\":\"674366098132566028\",\"userName\":\"virtu\
              llyvivek\"}],\"distance\":1.2352224588394165},{\"messages\":[{\"id\
              \":\"1238546488385339433\",\"content\":\"yup\",\"timestamp\":\"20\
              24-05-10
              17:41:33\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"}],\"distance\":1.2371547222137451},{\"messages\":[{\"id\":\"13\
              07802443161141249\",\"content\":\"I'm also thrilled with Rivet; it
              truly allows for a modular approach to complex tasks. It's
              particularly great that there's no need to rely heavily on the
              company's IT resources. Developers set up the API once, and then
              you can manage the entire pipeline independently. I would love to
              see more information from the Rivet developers about their vision
              for the future of this framework. It would also be great if there
              was a full commercial version available with a support
              subscription.\",\"timestamp\":\"2024-11-17
              20:20:00\",\"userId\":\"1071422157114060911\",\"userName\":\"maxii\
              \"},{\"id\":\"1308124580493131977\",\"content\":\"thank you both!
              ❤️\",\"timestamp\":\"2024-11-18
              17:40:03\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"}],\"distance\":1.23720121383667},{\"messages\":[{\"id\":\"1157\
              096352992481340\",\"content\":\"can you use local
              llms?\",\"timestamp\":\"2023-09-28
              23:27:48\",\"userId\":\"485597060867948545\",\"userName\":\"ThePe\
              rsonOfFun\"},{\"id\":\"1157156968318902323\",\"content\":\"yeah,
              using the oobabooga plugin, or any openai-api-compatible local
              LLM\",\"timestamp\":\"2023-09-29
              03:28:40\",\"userId\":\"110875316037091328\",\"userName\":\"Snea\
              \"},{\"id\":\"1168124163974770769\",\"content\":\"It seems like I
              can not add the plugin.. will post in the plugins
              section!\",\"timestamp\":\"2023-10-29
              09:48:23\",\"userId\":\"555530920480866310\",\"userName\":\"Asset\
              DK\"},{\"id\":\"1168173672180228106\",\"content\":\"I wasn't aware
              that such an option was available. we're interested in using the
              oobabooga plugin or a locally hosted OpenAI API-compatible
              language model. If you have experience or insights on how to set
              this up, your guidance would be greatly appreciated. Could you
              please share the steps or any relevant information that can help
              us get started with this? Thank you in advance for your assistance
              🥹\",\"timestamp\":\"2023-10-29
              13:05:07\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168178508342759454\",\"content\":\"The integration
              of a local LLM through a plugin would greatly enhance Rivet's
              capabilities, unlocking even more of its
              potential.\",\"timestamp\":\"2023-10-29
              13:24:20\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168179470901977119\",\"content\":\"I use LM Studio
              for local models with Rivet. If you search this discord for LM
              Studio you'll find a guide by me on how to set it up (it's really
              easy). Get back to me if you get stuck and I'll
              help\",\"timestamp\":\"2023-10-29
              13:28:09\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168180719319777332\",\"content\":\"Thank
              you for your response. I've noticed that LM Studio offers a
              user-friendly interface for running local models on my laptop. I'm
              curious if there's a way to establish a direct connection between
              Rivet's chat agent and LM Studio without relying on the OpenAI
              API. Is it possible to link them through a Port URL or a similar
              API, utilizing a locally hosted
              server?\",\"timestamp\":\"2023-10-29
              13:33:07\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168181082391334932\",\"content\":\"I have already
              set up LM Studio\",\"timestamp\":\"2023-10-29
              13:34:33\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168181512684961912\",\"content\":\"Yeah, it's just
              2 steps. this post is a feedback for the Rivet LMS documentation
              but it shold be all the info you need to set it up:
              https://discord.com/channels/1149376303070466110/1149376304756564\
              092/1163065106331074603\",\"timestamp\":\"2023-10-29
              13:36:16\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168181690494111765\",\"content\":\"Basical\
              ly just start the LMS server and change the endpoint in Rivet
              settings\",\"timestamp\":\"2023-10-29
              13:36:58\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168190877479469057\",\"content\":\"do I
              have here make everything right ?\",\"timestamp\":\"2023-10-29
              14:13:29\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168191251762384917\",\"content\":\"here is
              Rivet\",\"timestamp\":\"2023-10-29
              14:14:58\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168191673164111892\",\"content\":\"Yes, almost.
              Make `http://localhost:1238/v1/chat/completions` your endpoint in
              Rivet instead of
              `http://localhost:1238/v1`\",\"timestamp\":\"2023-10-29
              14:16:38\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168191691702939688\",\"content\":\"it looks
              like it stops in the chat\",\"timestamp\":\"2023-10-29
              14:16:43\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168192085929771028\",\"content\":\"And you also
              have 0 GPU layers with GPU acceleration on in LMS server. It will
              be very very slow. What GPU do you
              have?\",\"timestamp\":\"2023-10-29
              14:18:17\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168192481150631996\",\"content\":\"And
              you'll probably want to increase the context in LMS
              server\",\"timestamp\":\"2023-10-29
              14:19:51\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168194855814570094\",\"content\":\"so my
              friend, I have just make the modification that you have just
              provided, the local host :
              http://localhost:1238/v1/chat/completions, I increased the context
              to 4 k and the number of layers to
              10.\",\"timestamp\":\"2023-10-29
              14:29:17\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168195244081295420\",\"content\":\"Hmm\",\"timesta\
              mp\":\"2023-10-29
              14:30:50\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168195379888668762\",\"content\":\"I don't
              know if the llm will work with rivet with my AMD Radeo R7, it have
              already work but wlowly in the chat of LM
              Studio\",\"timestamp\":\"2023-10-29
              14:31:22\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168195461920858234\",\"content\":\"I'm running that
              on my laptop\",\"timestamp\":\"2023-10-29
              14:31:42\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168195639788720138\",\"content\":\"If you stop the
              sever, can you use chat in LMS with the same
              settings?\",\"timestamp\":\"2023-10-29
              14:32:24\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168195872740352121\",\"content\":\"Maybe
              remove the headers from the Rivet setting, I don't have any
              headers\",\"timestamp\":\"2023-10-29
              14:33:20\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168195964943732806\",\"content\":\"\",\"ti\
              mestamp\":\"2023-10-29
              14:33:42\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168195969305821284\",\"content\":\"\",\"timestamp\
              \":\"2023-10-29
              14:33:43\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168196113090748447\",\"content\":\"If it
              works in chat it also works in the
              server\",\"timestamp\":\"2023-10-29
              14:34:17\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168196151573495869\",\"content\":\"So the
              problem is in Rivet\",\"timestamp\":\"2023-10-29
              14:34:26\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168196657683370104\",\"content\":\"Oooh,
              something hapen\",\"timestamp\":\"2023-10-29
              14:36:27\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168196734443323482\",\"content\":\"it have now take
              a little bit longer, and the error
              changed\",\"timestamp\":\"2023-10-29
              14:36:45\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168197026715009034\",\"content\":\"\",\"timestamp\
              \":\"2023-10-29
              14:37:55\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168197184034963476\",\"content\":\"Should
              be here in server for you\",\"timestamp\":\"2023-10-29
              14:38:32\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168204556228902912\",\"content\":\"so my
              friend, by removing the headers the error have changed as
              mentionned. Then I have tiyed some modification, and restart
              computer and server and reload the model, it have taken some 10
              min processing, then the error failed to fetch
              happend\",\"timestamp\":\"2023-10-29
              15:07:50\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168208389080359084\",\"content\":\"That is so
              strange... Are you running Rivet
              1.5.4?\",\"timestamp\":\"2023-10-29
              15:23:04\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168208531888029697\",\"content\":\"Because
              many of us have had problems with that version. I downgraded to
              1.5.2 to solve my issues\",\"timestamp\":\"2023-10-29
              15:23:38\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168209933968355488\",\"content\":\"https:/\
              /github.com/Ironclad/rivet/releases/tag/app-v1.5.2\",\"timestamp\
              \":\"2023-10-29
              15:29:12\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168217130714533939\",\"content\":\"Oh my
              God !!! I'm absolutely thrilled to inform you that my initial
              response in Rivet's visual programming was successful, thanks to
              your invaluable assistance. Your guidance was instrumental in
              enabling me to achieve my first LM response in Rivet using the
              local LLM, and I truly appreciate your support throughout this
              process.\",\"timestamp\":\"2023-10-29
              15:57:48\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168217292748902520\",\"content\":\"I was trying to
              make it work for 3 days\",\"timestamp\":\"2023-10-29
              15:58:27\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168217383362625657\",\"content\":\"really thank
              you\",\"timestamp\":\"2023-10-29
              15:58:48\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"},{\"id\":\"1168220514033082419\",\"content\":\"Really glad that
              you got it working! Welcome to the
              community!\",\"timestamp\":\"2023-10-29
              16:11:15\",\"userId\":\"448864484631773187\",\"userName\":\"Egali\
              taristen\"},{\"id\":\"1168251744812290068\",\"content\":\"Thanks,
              I'm happy to improve the Rivet
              functionality\",\"timestamp\":\"2023-10-29
              18:15:21\",\"userId\":\"1053736378573266954\",\"userName\":\"Mag\
              \"}],\"distance\":1.2397295236587524}]"
          outgoingConnections:
            - output->"Graph Input" CjCP5tAVoFnV1_UgLHGsp/default
          visualData: -563.600023297057/419.0982796779662/230/43//
        '[FA7RjrEMsb2Sqbw1_sgbE]:text "Text"':
          data:
            schema: ""
            text: >-
              The following is a list of conversations that may be relevant to
              your answer:


              <conversations>

              {{conversations}}

              </conversations>


              The following is a tree of every file in the Rivet repository. It may be helpful for running commands to read file paths:


              <file_tree>

              {{file_tree}}

              </file_tree>



              The last 10 messages in the Rivet discord are:


              <last_10_messages>

              {{last_10}}

              </last_10_messages>


              *You are analyzing the LAST MESSAGE in the last 10 messages.* Use the other 9 messages for context, but your message in question is the *last one*.


              Can you answer that user's question based on the information you have been provided about Rivet?


              ## Continuing conversations


              The message you are analyzing may be part of a conversation two people are having. If so, don't butt in. Let them converse without interrupting them. Return a low helpfulness score.


              ## Direct Mentions


              Your Bot ID is: 1339820818250727434


              If the last message user tags this user specifically, you MUST reply, and try to be as useful as possible.


              Additionally, if the user if implicitly replying to you, the rivet bot, you are allowed to say something in response to that, too.


              ## Linking Conversations


              You may link to specific messages in the Rivet Discord (markdown link syntax). Links must follow the following format:


              https://discord.com/channels/1149376303070466110/:channel_id/:message_id


              In your links you MUST include all of the 


              * Main Server ID (above)

              * Channel ID (at the beginning of a <conversation> section

              * Message ID (with each message)


              This helps people build up trust that you are referencing things that actually happened.


              ## Linking Files


              You can link files in the Rivet repository using the following template:


              https://github.com/Ironclad/rivet/tree/main/:file_path


              For example:


              https://github.com/Ironclad/rivet/tree/main/packages/core/src/model/nodes/ChatNode.ts


              {{mention}}


              ## Functions


              You are provided with a series of functions to use to answer the user's question. You may call as many functions as you wish in order to better answer the user's question.


              For example, if the user is asking a question that can be answered by reading the source code of Rivet, you can use the `readRivetFile` function to see the contents of the source code file.
          outgoingConnections:
            - output->"Chat" bkk_WB9asbFyLV-4gg7hw/prompt
          visualData: 2207.0659660577403/726.6347887366494/330/17//
        '[HVWO5QwRH9wMWLLsuyqdq]:text "Text"':
          data:
            text: >-
              # Important


              You HAVE been mentioned via a direct @mention in Discord.


              You MUST answer as helpfully as possible based on the information you have been provided.


              Even if the questioner is Snea/Andy, you must still answer.


              You must reply with `shouldReply` set to `true` and `helpfulness` set to `10`.
          outgoingConnections:
            - output->"If/Else" kH12z7VW3csAj_4D12a-A/true
          visualData: 1144.0505400375082/1240.1049096762858/330/38//
        '[IAKoRoISKFkoLMI0AK52c]:text "Text"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Match" AD5oG59KEgJVykz_KHGG4/input
          visualData: 3422.3063417329663/402.5383795595691/330/71//
        '[IY1sv66H2GzKJZ6NsdxNf]:text "Text"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Extract JSON" qir_Y5dPVH1fbXTB_H2TB/input
          visualData: 7068.595613287889/38.63153732670161/330/79//
        '[NyN0tAEAs2ZbLt-0aJRGH]:object "Object"':
          data:
            jsonTemplate: "[{\"id\":\"1339428384970903663\",\"content\":\"ohh\",\"timestamp\
              \":\"2025-02-13T02:50:12.353Z\",\"user\":{\"id\":\"79345489421537\
              280\",\"displayName\":\"Grant\"},\"channelId\":\"1149382713904746\
              597\"},{\"id\":\"1339428456802549780\",\"content\":\"i didn't my
              aplogies, i've used that before but forgot it
              existed\",\"timestamp\":\"2025-02-13T02:50:29.479Z\",\"user\":{\"\
              id\":\"79345489421537280\",\"displayName\":\"Grant\"},\"channelId\
              \":\"1149382713904746597\"},{\"id\":\"1339428588516147232\",\"con\
              tent\":\"it
              works!\",\"timestamp\":\"2025-02-13T02:51:00.882Z\",\"user\":{\"id\
              \":\"79345489421537280\",\"displayName\":\"Grant\"},\"channelId\":\
              \"1149382713904746597\"},{\"id\":\"1340039589443801149\",\"content\
              \":\"This is stuck for me no matter what i
              enter\",\"timestamp\":\"2025-02-14T19:18:54.855Z\",\"user\":{\"id\
              \":\"321648551610417172\",\"displayName\":\"anurag\"},\"channelId\
              \":\"1149382713904746597\"},{\"id\":\"1340040034811908237\",\"con\
              tent\":\"and I'm on the latest version, there are no logs, error
              notification
              etc\",\"timestamp\":\"2025-02-14T19:20:41.039Z\",\"user\":{\"id\":\
              \"321648551610417172\",\"displayName\":\"anurag\"},\"channelId\":\
              \"1149382713904746597\"},{\"id\":\"1340051587279618069\",\"content\
              \":\"i'm getting this in the chat node with mistral api Error:
              Error processing ChatNode: OpenAIError: 422
              {\\\"object\\\":\\\"error\\\",\\\"message\\\":{\\\"detail\\\":[{\\\
              \"type\\\":\\\"extra_forbidden\\\",\\\"loc\\\":[\\\"body\\\",\\\"\
              stream_options\\\",\\\"include_usage\\\"],\\\"msg\\\":\\\"Extra
              inputs are not
              permitted\\\",\\\"input\\\":true,\\\"url\\\":\\\"https://errors.p\
              ydantic.dev/2.10/v/extra_forbidden\\\"}]},\\\"type\\\":\\\"invali\
              d_request_error\\\",\\\"param\\\":null,\\\"code\\\":null}\",\"tim\
              estamp\":\"2025-02-14T20:06:35.362Z\",\"user\":{\"id\":\"59536056\
              4662763662\",\"displayName\":\"elekt\"},\"channelId\":\"114938271\
              3904746597\"},{\"id\":\"1340051731261685860\",\"content\":\"can
              you open the F12 console and go to the network tab and see what
              it's
              doing?\",\"timestamp\":\"2025-02-14T20:07:09.690Z\",\"user\":{\"id\
              \":\"110875316037091328\",\"displayName\":\"Snea\"},\"replyTo\":\
              \"1340039589443801149\",\"channelId\":\"1149382713904746597\"},{\
              \"id\":\"1340051850946150492\",\"content\":\"ah... it doesn't
              support `stream_options`, that's annoying. I'll have to add
              something so you can disable various
              features\",\"timestamp\":\"2025-02-14T20:07:38.225Z\",\"user\":{\
              \"id\":\"110875316037091328\",\"displayName\":\"Snea\"},\"replyTo\
              \":\"1340051587279618069\",\"channelId\":\"1149382713904746597\"}\
              ,{\"id\":\"1340070098827874435\",\"content\":\"https://github.com\
              /cline/cline/issues/582 and
              https://github.com/cline/cline/issues/939\",\"timestamp\":\"2025-\
              02-14T21:20:08.859Z\",\"user\":{\"id\":\"110875316037091328\",\"d\
              isplayName\":\"Snea\"},\"replyTo\":\"1340051587279618069\",\"chan\
              nelId\":\"1149382713904746597\"},{\"id\":\"1340132322883801160\",\
              \"content\":\"hm\",\"timestamp\":\"2025-02-15T01:27:24.230Z\",\"u\
              ser\":{\"id\":\"595360564662763662\",\"displayName\":\"elekt\"},\
              \"channelId\":\"1149382713904746597\"}]"
          outgoingConnections:
            - output->"Graph Input" v37SmR9BGDJgDPq1b8qC-/default
          visualData: -30.161193976750667/1016.1468513357837/230/41//
        '[PN0B3V9xSaezm6wSE-1wj]:text "Text"':
          data:
            text: >-
              Rivet is a powerful Integrated Development Environment (IDE) and
              library designed for creating AI agents using a visual,
              graph-based interface. It is made up of two main components: the
              Rivet Application and the Rivet Core/Rivet Node. The Rivet
              Application is an editor that allows you to create complex AI
              prompt chains and agents. It comes with a suite of tools for
              designing and enhancing AI agents, such as a prompt designer,
              variations on nodes for A/B testing, and integrated testing to
              ensure your graphs work as expected for all inputs.


              Rivet Core/Rivet Node, on the other hand, are TypeScript libraries that allow you to execute projects generated by the Rivet Application. They provide a simple API for integrating Rivet with your application. This makes it easy to integrate Rivet's AI capabilities into your existing projects. Once you've created a graph in the Rivet App, you can execute it within your application like a function call.


              One of the key features of Rivet is its node-based editor. This enables you to create, configure, and debug complex AI prompt chains and AI agent chains visually. This approach makes it easier to understand the flow of data and the state of your AI agent at any point in time. The editor allows you to view the input and output of every node, as well as AI responses in real-time, making it simple to identify and fix issues.


              Rivet also offers live debugging of AI chains as they run, allowing you to monitor the state of your AI agent in real-time and quickly identify any issues that may arise. It also supports remote debugging, allowing you to debug AI chains running on a remote server. This is useful for debugging AI agents that are running in a production environment.


              Lastly, Rivet features a library of node types to execute common functionality for nodes. Some essential node types include Text, Chat, Match, Loop Controller, Extract YAML, Extract JSON, Chunk, Trim Chat Messages, and External Call. These nodes can be connected together using wires, allowing data to flow between them. Documentation for all nodes can be found in the Node Reference.
          outgoingConnections:
            - output->"Text" lqKtY94zAp7BCpPWng9fj/rivet
          visualData: 1790.9988422986908/210.75965552094894/330/15//
        '[QRNxeUUxBL_dyIVOfQZVy]:pop "Pop"':
          outgoingConnections:
            - lastItem->"Match" bIygMpK7e41Z6ZQ3N0gmF/input
          visualData: 735.5082845281257/1027.5186092945764/230/40//
        '[RQpZj3ZxF2I51mxp99XaN]:graphOutput "Graph Output"':
          data:
            dataType: number
            id: helpfulness
          visualData: 8462.243615716186/207.74378709519183/330/79//
        '[Uss0ThurAIE3ZOzkt3XHA]:join "Join"':
          data:
            flatten: true
            joinString: "\n\n"
          outgoingConnections:
            - output->"Text" FA7RjrEMsb2Sqbw1_sgbE/conversations
          visualData: 1429.8048033359498/619.5592530014424/180/42//
        '[WGYV__c0cf7fKhAEXb5Xi]:delegateFunctionCall "Delegate Function Call"':
          data:
            handlers:
              - key: replyToUser
                value: 7iHnWEmcH0QvWCxWPIdxa
          isSplitRun: true
          outgoingConnections:
            - output->"Text" IY1sv66H2GzKJZ6NsdxNf/input
          visualData: 6448.738053579676/139.18154477869496/442.9055576925157/80//
        '[XLZkevyQOr_C25B04D0-T]:text "Text"':
          data:
            text: ""
          outgoingConnections:
            - output->"If/Else" kH12z7VW3csAj_4D12a-A/false
          visualData: 1174.6314355238658/1536.733058294768/330/39//
        '[YndaLL2AGMve9B3tIXh_g]:readDirectory "Read Directory"':
          data:
            filterGlobs: []
            ignores: []
            includeDirectories: false
            path: /usr/local/repos/rivet/packages/core/src
            recursive: true
            relative: false
            useFilterGlobsInput: false
            useIgnoresInput: false
            useIncludeDirectoriesInput: false
            usePathInput: false
            useRecursiveInput: false
            useRelativeInput: false
          outgoingConnections:
            - tree->"Extract Object Path" BuZ9gxK8bT5QbnTEQFx_x/object
          visualData: -302.98967922158306/135.020396693047/230/60//
        '[_Eurz84lHU5ynlP_-e4rO]:subGraph "Subgraph"':
          data:
            graphId: DYKDBueiTSnn9xhpjq6Cb
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - output->"Text" FA7RjrEMsb2Sqbw1_sgbE/last_10
          visualData: 663.048084824735/1303.8377349919783/330/24//
        '[_N9pD3_tIORVGCL7A0omw]:extractObjectPath "Extract Object Path"':
          data:
            path: $.messages[*]
            usePathInput: false
          isSplitRun: true
          outgoingConnections:
            - all_matches->"Subgraph" d8yzrLcYgx-yvY9ESKdXb/messages
          visualData: 169.41549454683246/616.0853481413917/464/42//
        '[bIygMpK7e41Z6ZQ3N0gmF]:match "Match"':
          data:
            cases:
              - "1339820818250727434"
            text: ""
          outgoingConnections:
            - case1->"If/Else" kH12z7VW3csAj_4D12a-A/if
          visualData: 1206.5808592151689/1012.83124958825/280/37//
        '[bkk_WB9asbFyLV-4gg7hw]:chat "Chat"':
          data:
            additionalParameters: []
            cache: false
            enableFunctionUse: true
            headers: []
            maxTokens: 1024
            modalitiesIncludeAudio: false
            modalitiesIncludeText: false
            model: gpt-4o-mini
            outputUsage: false
            parallelFunctionCalling: true
            reasoningEffort: medium
            responseFormat: json
            stop: ""
            temperature: 0.2
            text: ""
            toolChoice: auto
            top_p: 1
            useAdditionalParametersInput: false
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePredictedOutput: false
            usePresencePenaltyInput: false
            useReasoningEffortInput: false
            useServerTokenCalculation: true
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - all-messages->"Assemble Prompt" rOL6VbCiarjjnfrSnJ2Si/message1
            - function-calls->"Match" AD5oG59KEgJVykz_KHGG4/value
            - function-calls->"To JSON" kpXVr9ARkZW6o-nPot7GZ/data
          visualData: 2891.2919585979116/698.9771475619148/230/14//
        '[d0mVbF4PrtGDYaT70HrdJ]:coalesce "Coalesce"':
          outgoingConnections:
            - output->"Delegate Function Call"
              WGYV__c0cf7fKhAEXb5Xi/function-call
          visualData: 6026.430088103376/605.969867107758/180/null//
        '[d8yzrLcYgx-yvY9ESKdXb]:subGraph "Subgraph"':
          data:
            graphId: DYKDBueiTSnn9xhpjq6Cb
            useAsGraphPartialOutput: false
            useErrorOutput: false
          isSplitRun: true
          outgoingConnections:
            - output->"Text" lFD3AerRHkh_A1uXf37D_/conversation
          splitRunMax: 1000
          visualData: 684.2509977561499/630.3818082437297/330/42//
        '[grjGx_dLGo-EbghJwxXHr]:array "Array"':
          data:
            flatten: true
            flattenDeep: false
          outgoingConnections:
            - output->"Chat" stve7WsObo-_M_-NcVgQw/functions
          visualData: 2775.654076996353/1562.507498348418/230/null//
        '[kH12z7VW3csAj_4D12a-A]:ifElse "If/Else"':
          data:
            unconnectedControlFlowExcluded: true
          outgoingConnections:
            - output->"Text" FA7RjrEMsb2Sqbw1_sgbE/mention
          visualData: 1661.1281793912403/1080.1715933180385/205/null//
        '[kpXVr9ARkZW6o-nPot7GZ]:toJson "To JSON"':
          data:
            indented: false
          isSplitRun: true
          outgoingConnections:
            - json->"Text" IAKoRoISKFkoLMI0AK52c/input
          visualData: 3175.7925834364196/398.9308611454733/205/69//
        '[lFD3AerRHkh_A1uXf37D_]:text "Text"':
          data:
            text: |-
              <conversation>
              {{conversation}}
              </conversation>
          isSplitRun: true
          outgoingConnections:
            - output->"Join" Uss0ThurAIE3ZOzkt3XHA/input1
          visualData: 1055.8253944080177/607.534191621123/330/42//
        '[leQ7noO3cvDfUMhsUmKJ9]:graphOutput "Graph Output"':
          data:
            dataType: boolean
            id: shouldReply
          visualData: 8458.636097302091/394.1322384901419/330/79//
        '[lqKtY94zAp7BCpPWng9fj]:text "Text"':
          data:
            text: >-
              {{rivet}}


              You are an AI assitant bot for Rivet known as Rivet Bot.


              You reply as if you are in a chat room. That means very short messages. You are informal. You do not capitalize the start of your message. However you do capitalize after periods.


              Your replies are very short, informal, like you are having quick conversations in a Discord chat room.


              You only answer questions that are answerable using past conversations, which are provided to you.


              You do not use emoji.


              You do not try to be helpful or say things like "just ask!". You try to be helpful with factual information that you can discern from previous messages, but that is is.


              You do not start your replies with the word "hey". You just jump right into your answer.


              ## Important notes:


              Andy (Snea) is the creator of Rivet. What he said should be treated as authoritative when it comes to Rivet. Every other user is just a user of Rivet asking questions or providing input/conversation in discord. If Andy/Snea replies, that should be seen as authoritative statements about Rivet.


              If Andy is the last replier, you cannot be helpful.


              ## Replies


              You always reply by calling the functions you have been provided, with valid JSON input.
          outgoingConnections:
            - output->"Chat" bkk_WB9asbFyLV-4gg7hw/systemPrompt
            - output->"Chat" stve7WsObo-_M_-NcVgQw/systemPrompt
          visualData: 2489.6549084952453/269.68245628451376/330/21//
        '[nYZj0Kq7IrQgJ6s5Vd1qu]:destructure "Destructure"':
          data:
            paths:
              - $.internalThoughts
              - $.reply
              - $.helpfulness
              - $.shouldReply
          outgoingConnections:
            - match_0->"Graph Output" 3GdlHogl2PM56pjMg6CEx/value
            - match_1->"Graph Output" zp5NY2aly11KTUH9xjTW3/value
            - match_2->"Graph Output" RQpZj3ZxF2I51mxp99XaN/value
            - match_3->"Graph Output" leQ7noO3cvDfUMhsUmKJ9/value
          visualData: 7881.433151046764/0.9127313536989039/280/79//
        '[poJFOQ2N1oIlq8-1C0xFN]:array "Array"':
          data:
            flatten: true
            flattenDeep: false
          outgoingConnections:
            - output->"Chat" bkk_WB9asbFyLV-4gg7hw/functions
          visualData: 2557.4450595506873/1183.4378589362645/230/47//
        '[qir_Y5dPVH1fbXTB_H2TB]:extractJson "Extract JSON"':
          outgoingConnections:
            - output->"Destructure" nYZj0Kq7IrQgJ6s5Vd1qu/object
          visualData: 7502.643717566703/2.1152374917308396/280/79//
        '[rOL6VbCiarjjnfrSnJ2Si]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Chat" stve7WsObo-_M_-NcVgQw/prompt
          visualData: 5148.046095561043/865.3629704802104/280/78//
        '[stve7WsObo-_M_-NcVgQw]:chat "Chat"':
          data:
            additionalParameters: []
            cache: false
            enableFunctionUse: true
            headers: []
            maxTokens: 1024
            modalitiesIncludeAudio: false
            modalitiesIncludeText: false
            model: gpt-4o-mini
            outputUsage: false
            parallelFunctionCalling: true
            reasoningEffort: medium
            stop: ""
            temperature: 0.2
            toolChoice: auto
            top_p: 1
            useAdditionalParametersInput: false
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePredictedOutput: false
            usePresencePenaltyInput: false
            useReasoningEffortInput: false
            useServerTokenCalculation: true
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - function-calls->"Coalesce" d0mVbF4PrtGDYaT70HrdJ/input2
          visualData: 5604.931774841902/751.1415506599955/230/77//
        '[v37SmR9BGDJgDPq1b8qC-]:graphInput "Graph Input"':
          data:
            dataType: object[]
            defaultValue: ""
            id: messages
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Pop" QRNxeUUxBL_dyIVOfQZVy/array
            - data->"Subgraph" _Eurz84lHU5ynlP_-e4rO/messages
          visualData: 271.0310838263239/1272.956556489847/330/22//
        '[yNvvuc19rl-I6kkvVIltc]:gptFunction "GPT Function"':
          data:
            description: >-
              Reads the contents of the specified file in Rivet. This is useful
              if questions can be answered by reading the source code of Rivet.
              The path should be relative, and start with `packages`.


              If you are in doubt, read one or more files in Rivet so that you understand Rivet better before replying to the user! Reading files is quick and easy. You can read multiple files by calling this function more than once at a time.
            name: readRivetFile
            schema: |-
              
              {
                "type": "object",
                "properties": {
                  "path": {
                    "type": "string",
                    "description": "The file path of the file to read"
                  }
                },
                "required": ["path"],
                "additionalProperties": false
              }
          outgoingConnections:
            - function->"Array" poJFOQ2N1oIlq8-1C0xFN/input2
          visualData: 2149.2372530793277/1847.8856542695976/280/64//
        '[zp5NY2aly11KTUH9xjTW3]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: reply
          visualData: 8452.623566611932/-17.12486071678002/330/79//
    kMImnA_JwET5paPQ7hHN4:
      metadata:
        description: ""
        id: kMImnA_JwET5paPQ7hHN4
        name: Embed Query
      nodes:
        '[7cHKUKW7PuR0hzPRbAWqj]:text "Text"':
          data:
            text: >-
              Rivet is a powerful Integrated Development Environment (IDE) and
              library designed for creating AI agents using a visual,
              graph-based interface. It is made up of two main components: the
              Rivet Application and the Rivet Core/Rivet Node. The Rivet
              Application is an editor that allows you to create complex AI
              prompt chains and agents. It comes with a suite of tools for
              designing and enhancing AI agents, such as a prompt designer,
              variations on nodes for A/B testing, and integrated testing to
              ensure your graphs work as expected for all inputs.


              Rivet Core/Rivet Node, on the other hand, are TypeScript libraries that allow you to execute projects generated by the Rivet Application. They provide a simple API for integrating Rivet with your application. This makes it easy to integrate Rivet's AI capabilities into your existing projects. Once you've created a graph in the Rivet App, you can execute it within your application like a function call.


              One of the key features of Rivet is its node-based editor. This enables you to create, configure, and debug complex AI prompt chains and AI agent chains visually. This approach makes it easier to understand the flow of data and the state of your AI agent at any point in time. The editor allows you to view the input and output of every node, as well as AI responses in real-time, making it simple to identify and fix issues.


              Rivet also offers live debugging of AI chains as they run, allowing you to monitor the state of your AI agent in real-time and quickly identify any issues that may arise. It also supports remote debugging, allowing you to debug AI chains running on a remote server. This is useful for debugging AI agents that are running in a production environment.


              Lastly, Rivet features a library of node types to execute common functionality for nodes. Some essential node types include Text, Chat, Match, Loop Controller, Extract YAML, Extract JSON, Chunk, Trim Chat Messages, and External Call. These nodes can be connected together using wires, allowing data to flow between them. Documentation for all nodes can be found in the Node Reference.
          outgoingConnections:
            - output->"Text" pLkEsbnz6BBGFU0L0Eou6/rivet
          visualData: -547/201/330/15//
        '[OIWOxFo3s-nF7G5NK8e65]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: rephrased
          visualData: 1044/713/330/19//
        '[QHlKKB0LOa7BrYasbux2D]:graphInput "Graph Input"':
          data:
            dataType: object[]
            id: messages
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Subgraph" UEWPAqgN-FtA1CvLZvtg7/messages
          visualData: -950/527/330/6//
        '[UEWPAqgN-FtA1CvLZvtg7]:subGraph "Subgraph"':
          data:
            graphId: DYKDBueiTSnn9xhpjq6Cb
            useAsGraphPartialOutput: false
            useErrorOutput: false
          outgoingConnections:
            - output->"Text" pLkEsbnz6BBGFU0L0Eou6/last_messages
          visualData: -543/543/330/18//
        '[X8ZdjoBE-DcfKguapR4H8]:chat "Chat"':
          data:
            additionalParameters: []
            cache: false
            enableFunctionUse: false
            headers: []
            maxTokens: 1024
            modalitiesIncludeAudio: false
            modalitiesIncludeText: false
            model: gpt-4o-mini
            outputUsage: false
            parallelFunctionCalling: true
            reasoningEffort: medium
            stop: ""
            temperature: 0
            top_p: 1
            useAdditionalParametersInput: false
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePredictedOutput: false
            usePresencePenaltyInput: false
            useReasoningEffortInput: false
            useServerTokenCalculation: true
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Get Embedding" gemYFnDfNtr-q6z9fSlSN/input
            - response->"Graph Output" OIWOxFo3s-nF7G5NK8e65/value
          visualData: 326/429/230/17//
        '[_M0CdAx1uCUnIf786OjB8]:text "Text"':
          data:
            text: You are Rivet Bot, with an ID of 1339820818250727434.
          outgoingConnections:
            - output->"Chat" X8ZdjoBE-DcfKguapR4H8/systemPrompt
          visualData: -106.83151068615273/-18.174078121373753/330/23//
        '[gemYFnDfNtr-q6z9fSlSN]:getEmbedding "Get Embedding"':
          data:
            integration: openai
            model: text-embedding-3-small
            useIntegrationInput: false
          outgoingConnections:
            - embedding->"Graph Output" gmZnRxUZgZIhWBCwiBMQ7/value
          visualData: 696/541/280/3//
        '[gmZnRxUZgZIhWBCwiBMQ7]:graphOutput "Graph Output"':
          data:
            dataType: vector
            id: output
          visualData: 1042/528/330/4//
        '[pLkEsbnz6BBGFU0L0Eou6]:text "Text"':
          data:
            text: >-
              {{rivet}}


              The follow are the last 10 messages to the Rivet Discord:


              <last_messages>

              {{last_messages}}

              </last_messages>


              You are analyzing the last message in this list. 


              You must rephrase the last message so that it does not require any other message in this list in order to understand its meaning. That means that if someone were to read this message by itself, they would understand what it means, even if they didn't see the context where the message was placed.


              Also, you must make sure there are a bunch of keyboards in your rephrasing so that the vector database can correctly find relevant information for the user that posted their message. However you don't need to use words like "Rivet" because the database is only populated with information about Rivet.


              Reply with your rephrased message.


              For example,


              1. Finding X

              2. Try again


              becomes "Try finding X again"


              For example,


              1. I need help

              2. I can't find Y

              3. help me please


              becomes "Help me, I can't find Y"


              If this message is a followup message to a previous message, include the contents of the previous message.
          outgoingConnections:
            - output->"Chat" X8ZdjoBE-DcfKguapR4H8/prompt
          visualData: -126/253/330/16//
    kW3AkPipD9r6PklOmeYCQ:
      metadata:
        description: ""
        id: kW3AkPipD9r6PklOmeYCQ
        name: Get Parent Message
      nodes:
        '[5SHWcScJF0M-gw0c3_UgZ]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 2810.7368250947034/619.1630308304016/330/57//
        '[APfA45HUT0MmeMZiFAmge]:text "Text"':
          data:
            text: >-
              {{user_name}} said on {{date}} with a message ID of
              {{message_id}}:


              """

              {{content}}

              """
          isSplitRun: true
          outgoingConnections:
            - output->"Text" sTbP1N607sIq4erKnNe0_/last_100
          splitRunMax: 99
          visualData: 859/486/330/50//
        '[DGxDyYVYWy6UxcIynpsJM]:graphInput "Graph Input"':
          data:
            dataType: object[]
            id: messages
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Pop" YR8D6XUrOliQ5kcwhhLo-/array
          visualData: -246.61770724508096/668.3346104046797/330/45//
        '[EAXK-mY_58m6kNmlwNOLZ]:destructure "Destructure"':
          data:
            paths:
              - $.id
              - $.content
              - $.timestamp
              - $.user.displayName
          outgoingConnections:
            - match_0->"Text" d_FcCYmR_VJsqSkEe_RMG/message_id
            - match_1->"Text" d_FcCYmR_VJsqSkEe_RMG/content
            - match_2->"Text" d_FcCYmR_VJsqSkEe_RMG/date
            - match_3->"Text" d_FcCYmR_VJsqSkEe_RMG/user_name
          visualData: 526/841/280/53//
        '[MkJMVWHv6HLCemwDs_gmP]:chat "Chat"':
          data:
            additionalParameters: []
            cache: false
            enableFunctionUse: false
            headers: []
            maxTokens: 1024
            modalitiesIncludeAudio: false
            modalitiesIncludeText: false
            model: gpt-4o-mini
            outputUsage: false
            parallelFunctionCalling: true
            reasoningEffort: medium
            stop: ""
            temperature: 0
            top_p: 1
            useAdditionalParametersInput: false
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePredictedOutput: false
            usePresencePenaltyInput: false
            useReasoningEffortInput: false
            useServerTokenCalculation: true
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Extract Regex" vOwmv2wQWmHm3i7P0WEJY/input
          visualData: 2092/561/230/58//
        '[Vu0jAlYZmwgMBTOE42NLi]:destructure "Destructure"':
          data:
            paths:
              - $.id
              - $.content
              - $.timestamp
              - $.user.displayName
          isSplitRun: true
          outgoingConnections:
            - match_0->"Text" APfA45HUT0MmeMZiFAmge/message_id
            - match_1->"Text" APfA45HUT0MmeMZiFAmge/content
            - match_2->"Text" APfA45HUT0MmeMZiFAmge/date
            - match_3->"Text" APfA45HUT0MmeMZiFAmge/user_name
          splitRunMax: 99
          visualData: 517/550/280/49//
        '[YR8D6XUrOliQ5kcwhhLo-]:pop "Pop"':
          outgoingConnections:
            - lastItem->"Destructure" EAXK-mY_58m6kNmlwNOLZ/object
            - restOfArray->"Destructure" Vu0jAlYZmwgMBTOE42NLi/object
          visualData: 142.59282925166804/681.6682194936012/230/46//
        '[bSehvhsIwRS1BoqlBjyCW]:text "Text"':
          data:
            text: >-
              Rivet is a powerful Integrated Development Environment (IDE) and
              library designed for creating AI agents using a visual,
              graph-based interface. It is made up of two main components: the
              Rivet Application and the Rivet Core/Rivet Node. The Rivet
              Application is an editor that allows you to create complex AI
              prompt chains and agents. It comes with a suite of tools for
              designing and enhancing AI agents, such as a prompt designer,
              variations on nodes for A/B testing, and integrated testing to
              ensure your graphs work as expected for all inputs.


              Rivet Core/Rivet Node, on the other hand, are TypeScript libraries that allow you to execute projects generated by the Rivet Application. They provide a simple API for integrating Rivet with your application. This makes it easy to integrate Rivet's AI capabilities into your existing projects. Once you've created a graph in the Rivet App, you can execute it within your application like a function call.


              One of the key features of Rivet is its node-based editor. This enables you to create, configure, and debug complex AI prompt chains and AI agent chains visually. This approach makes it easier to understand the flow of data and the state of your AI agent at any point in time. The editor allows you to view the input and output of every node, as well as AI responses in real-time, making it simple to identify and fix issues.


              Rivet also offers live debugging of AI chains as they run, allowing you to monitor the state of your AI agent in real-time and quickly identify any issues that may arise. It also supports remote debugging, allowing you to debug AI chains running on a remote server. This is useful for debugging AI agents that are running in a production environment.


              Lastly, Rivet features a library of node types to execute common functionality for nodes. Some essential node types include Text, Chat, Match, Loop Controller, Extract YAML, Extract JSON, Chunk, Trim Chat Messages, and External Call. These nodes can be connected together using wires, allowing data to flow between them. Documentation for all nodes can be found in the Node Reference.
          outgoingConnections:
            - output->"Text" sTbP1N607sIq4erKnNe0_/rivet
          visualData: 1217/270/330/4//
        '[d_FcCYmR_VJsqSkEe_RMG]:text "Text"':
          data:
            text: >-
              * {{user_name}} said on {{date}} with a message ID of
              {{message_id}}:


              """

              {{content}}

              """
          outgoingConnections:
            - output->"Text" sTbP1N607sIq4erKnNe0_/message
          visualData: 1017/815/330/52//
        '[hW41OZYUvfreGEcPCAzrr]:object "Object"':
          data:
            jsonTemplate: "[]"
          outgoingConnections:
            - output->"Graph Input" DGxDyYVYWy6UxcIynpsJM/default
          visualData: -630.5217135186575/614.5422175011613/230/59//
        '[sTbP1N607sIq4erKnNe0_]:text "Text"':
          data:
            text: >-
              {{rivet}}


              The following is a list of messages in the Rivet Discord server:


              <last_100_messages>

              {{last_100}}

              </last_100_messages>


              I am asking a question about this message below, which immediately follows the above messages:


              <message>

              {{message}}

              </message>


              My question is: is this message part of a conversation? 


              A conversation being defined as two people talking with each other, or replying to each other, or just continuing a conversation with yourself, like followup messages.


              A followup message from the same person is not necessarily part of the same conversation. The user may be starting a "new" conversation, or asking a "new" question, that is not part of the conversation before it.


              If this conversation is a reply to a previous message, reply with the ID of the previous message. Even if there is no `replyTo` field on the message, the message may still be continuing a conversation, just without explicitly replying to the user.


              If this conversation is not a reply to a previous message, reply NULL.


              Only reply with the previous message ID, or NULL.


              Reply inside <previousMessage> tags, for example:


              <previousMessage>123456</previousMessage>


              <previousMessage>NULL</previousMessage>


              Do not reply with anything besides <previousMessage>.
          outgoingConnections:
            - output->"Chat" MkJMVWHv6HLCemwDs_gmP/prompt
          visualData: 1662.5376073689497/406.8983991182554/330/51//
        '[vOwmv2wQWmHm3i7P0WEJY]:extractRegex "Extract Regex"':
          data:
            errorOnFailed: false
            multilineMode: false
            regex: <previousMessage>(.*?)<\/previousMessage>
            useRegexInput: false
          outgoingConnections:
            - output1->"Graph Output" 5SHWcScJF0M-gw0c3_UgZ/value
          visualData: 2452.632408467435/609.3910897187651/280/56//
    qjZ8pqtpef9Y0qtsiVzum:
      metadata:
        description: ""
        id: qjZ8pqtpef9Y0qtsiVzum
        name: "Function Call: readRivetFile"
      nodes:
        '[GbjVUI4R_c4MjZXgwLjlX]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1012/527/330/4//
        '[HumxEURbglkjqZt7NqqjH]:graphInput "Graph Input"':
          data:
            dataType: string
            id: path
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Text" QzJGuFm_bGGNNjHcNhrjY/path
          visualData: -261/580/330/8//
        '[QzJGuFm_bGGNNjHcNhrjY]:text "Text"':
          data:
            text: /usr/local/repos/rivet/{{path}}
          outgoingConnections:
            - output->"Read File" pmZzqLtqDdWwXDoAUY4aU/path
          visualData: 182/574/330/6//
        '[pmZzqLtqDdWwXDoAUY4aU]:readFile "Read File"':
          data:
            asBinary: false
            errorOnMissingFile: false
            path: ""
            usePathInput: true
          outgoingConnections:
            - content->"Graph Output" GbjVUI4R_c4MjZXgwLjlX/value
          visualData: 631/536/280/3//
  metadata:
    description: ""
    id: YRz-Gl8z4UnZ6Qe_ILR15
    title: Discord Bot
  plugins:
    - id: anthropic
      name: Anthropic
      type: built-in
